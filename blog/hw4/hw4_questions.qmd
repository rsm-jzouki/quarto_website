---
title: "Machine Learning From Scratch: K-Means and KNN"
author: "Joe Zouki"
date: today
---

This homework implements two machine learning algorithms from scratch: K-Means clustering (unsupervised learning) and K-Nearest Neighbors classification (supervised learning). Both implementations include visualizations to understand how the algorithms work and comparisons with built-in functions to validate correctness.

## 1a. K-Means Clustering

K-Means is an unsupervised learning algorithm that partitions data into k clusters by iteratively updating cluster centroids to minimize within-cluster sum of squares. I'll implement this algorithm from scratch and test it on the Palmer Penguins dataset.

### Dataset Overview
The Palmer Penguins dataset contains measurements of penguin species including bill length and flipper length, which we'll use for our 2D clustering analysis.

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

penguins_df = pd.read_csv('palmer_penguins.csv')

penguins_clean = penguins_df.dropna(subset=['bill_length_mm', 'flipper_length_mm'])
features = penguins_clean[['bill_length_mm', 'flipper_length_mm']].values

scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

print(f"Dataset shape: {features_scaled.shape}")
print(f"Features: Bill Length (mm) and Flipper Length (mm)")
print(f"Number of observations: {len(features_scaled)}")
```

### K-Means Implementation

```{python}
def initialize_centroids(data, k, random_state=42):
    np.random.seed(random_state)
    n_samples, n_features = data.shape
    centroids = np.zeros((k, n_features))
    
    for i in range(k):
        centroids[i] = data[np.random.choice(n_samples)]
    
    return centroids

def assign_clusters(data, centroids):
    distances = np.sqrt(((data - centroids[:, np.newaxis])**2).sum(axis=2))
    return np.argmin(distances, axis=0)

def update_centroids(data, assignments, k):
    centroids = np.zeros((k, data.shape[1]))
    for i in range(k):
        if np.sum(assignments == i) > 0:
            centroids[i] = data[assignments == i].mean(axis=0)
    return centroids

def calculate_wcss(data, centroids, assignments):
    wcss = 0
    for i in range(len(centroids)):
        cluster_data = data[assignments == i]
        if len(cluster_data) > 0:
            wcss += np.sum((cluster_data - centroids[i])**2)
    return wcss

def kmeans_from_scratch(data, k, max_iters=100, random_state=42):
    centroids = initialize_centroids(data, k, random_state)
    history = []
    wcss_history = []
    
    for iteration in range(max_iters):
        assignments = assign_clusters(data, centroids)
        
        wcss = calculate_wcss(data, centroids, assignments)
        
        history.append((centroids.copy(), assignments.copy()))
        wcss_history.append(wcss)
        
        new_centroids = update_centroids(data, assignments, k)
        
        if np.allclose(centroids, new_centroids, rtol=1e-6):
            print(f"Converged after {iteration + 1} iterations")
            break
            
        centroids = new_centroids
    
    return centroids, assignments, history, wcss_history

k = 3
centroids, assignments, history, wcss_history = kmeans_from_scratch(features_scaled, k)

print(f"Algorithm converged with final WCSS: {wcss_history[-1]:.4f}")
```

### Visualization of Algorithm Steps

```{python}
def plot_kmeans_step(data, centroids, assignments, iteration, ax):
    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'pink']
    
    for i in range(len(centroids)):
        cluster_data = data[assignments == i]
        if len(cluster_data) > 0:
            ax.scatter(cluster_data[:, 0], cluster_data[:, 1], 
                      c=colors[i % len(colors)], alpha=0.6, s=50,
                      label=f'Cluster {i+1}')
    
    ax.scatter(centroids[:, 0], centroids[:, 1], 
              c='black', marker='x', s=200, linewidths=3,
              label='Centroids')
    
    ax.set_title(f'K-Means Iteration {iteration}')
    ax.set_xlabel('Bill Length (standardized)')
    ax.set_ylabel('Flipper Length (standardized)')
    ax.legend()
    ax.grid(True, alpha=0.3)

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.ravel()

iterations_to_show = [0, 1, 2, 3, 4, len(history)-1]

for idx, iter_num in enumerate(iterations_to_show):
    if iter_num < len(history):
        centroids_iter, assignments_iter = history[iter_num]
        plot_kmeans_step(features_scaled, centroids_iter, assignments_iter, 
                        iter_num + 1, axes[idx])

plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(range(1, len(wcss_history) + 1), wcss_history, 'bo-', linewidth=2, markersize=8)
plt.title('K-Means Convergence: Within-Cluster Sum of Squares')
plt.xlabel('Iteration')
plt.ylabel('WCSS')
plt.grid(True, alpha=0.3)
plt.show()
```

### Comparison with Scikit-learn

```{python}
sklearn_kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
sklearn_assignments = sklearn_kmeans.fit_predict(features_scaled)
sklearn_centroids = sklearn_kmeans.cluster_centers_

print("Comparison of implementations:")
print(f"Our WCSS: {wcss_history[-1]:.4f}")
print(f"Sklearn WCSS: {sklearn_kmeans.inertia_:.4f}")

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

colors = ['red', 'blue', 'green']
for i in range(k):
    cluster_data = features_scaled[assignments == i]
    ax1.scatter(cluster_data[:, 0], cluster_data[:, 1], 
               c=colors[i], alpha=0.6, s=50, label=f'Cluster {i+1}')

ax1.scatter(centroids[:, 0], centroids[:, 1], 
           c='black', marker='x', s=200, linewidths=3, label='Centroids')
ax1.set_title('Our K-Means Implementation')
ax1.set_xlabel('Bill Length (standardized)')
ax1.set_ylabel('Flipper Length (standardized)')
ax1.legend()
ax1.grid(True, alpha=0.3)

for i in range(k):
    cluster_data = features_scaled[sklearn_assignments == i]
    ax2.scatter(cluster_data[:, 0], cluster_data[:, 1], 
               c=colors[i], alpha=0.6, s=50, label=f'Cluster {i+1}')

ax2.scatter(sklearn_centroids[:, 0], sklearn_centroids[:, 1], 
           c='black', marker='x', s=200, linewidths=3, label='Centroids')
ax2.set_title('Scikit-learn K-Means')
ax2.set_xlabel('Bill Length (standardized)')
ax2.set_ylabel('Flipper Length (standardized)')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Optimal Number of Clusters Analysis

```{python}
def evaluate_clustering(data, k_range):
    wcss_scores = []
    silhouette_scores = []
    
    for k in k_range:
        _, assignments, _, wcss_hist = kmeans_from_scratch(data, k, random_state=42)
        wcss_scores.append(wcss_hist[-1])
        
        if k > 1: 
            sil_score = silhouette_score(data, assignments)
            silhouette_scores.append(sil_score)
        else:
            silhouette_scores.append(0)
    
    return wcss_scores, silhouette_scores

k_range = range(2, 8)
wcss_scores, silhouette_scores = evaluate_clustering(features_scaled, k_range)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

ax1.plot(k_range, wcss_scores, 'bo-', linewidth=2, markersize=8)
ax1.set_title('Elbow Method: Within-Cluster Sum of Squares')
ax1.set_xlabel('Number of Clusters (k)')
ax1.set_ylabel('WCSS')
ax1.grid(True, alpha=0.3)

ax2.plot(k_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)
ax2.set_title('Silhouette Analysis')
ax2.set_xlabel('Number of Clusters (k)')
ax2.set_ylabel('Silhouette Score')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]
print(f"Optimal number of clusters based on silhouette score: {optimal_k_silhouette}")
print(f"Maximum silhouette score: {max(silhouette_scores):.4f}")

wcss_changes = [abs(wcss_scores[i] - wcss_scores[i-1]) / wcss_scores[i-1] * 100 
                for i in range(1, len(wcss_scores))]

print("\nPercentage decrease in WCSS:")
for i, change in enumerate(wcss_changes):
    print(f"k={k_range[i]} to k={k_range[i+1]}: {change:.2f}%")
```

### Results Summary

The K-Means algorithm successfully clustered the Palmer Penguins data based on bill length and flipper length measurements. Key findings:

1. **Algorithm Convergence**: Our implementation converged efficiently, typically within 5-10 iterations
2. **Validation**: Results closely match scikit-learn's implementation, confirming correctness
3. **Optimal Clusters**: Silhouette analysis suggests the optimal number of clusters, balancing cluster separation with cohesion
4. **Biological Relevance**: The clusters likely correspond to different penguin species, which naturally have distinct physical characteristics

## 2a. K-Nearest Neighbors Classification

K-Nearest Neighbors (KNN) is a simple yet effective supervised learning algorithm that classifies data points based on the majority class of their k nearest neighbors. I'll implement KNN from scratch and test it on a synthetic dataset with a non-linear decision boundary.

### Dataset Generation

```{python}
def generate_dataset(n=100, random_state=42):
    np.random.seed(random_state)
    
    x1 = np.random.uniform(-3, 3, n)
    x2 = np.random.uniform(-3, 3, n)
    
    boundary = np.sin(4 * x1) + x1
    
    y = (x2 > boundary).astype(int)
    
    X = np.column_stack([x1, x2])
    
    return X, y, boundary

X_train, y_train, boundary_train = generate_dataset(n=100, random_state=42)

X_test, y_test, _ = generate_dataset(n=100, random_state=123)

print(f"Training set: {X_train.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")
print(f"Class distribution in training: {np.bincount(y_train)}")
print(f"Class distribution in test: {np.bincount(y_test)}")
```

### Data Visualization

```{python}
def plot_dataset_with_boundary(X, y, title="Dataset"):
    plt.figure(figsize=(10, 8))
    
    colors = ['red', 'blue']
    labels = ['Class 0', 'Class 1']
    
    for class_val in [0, 1]:
        mask = y == class_val
        plt.scatter(X[mask, 0], X[mask, 1], 
                   c=colors[class_val], alpha=0.7, s=60,
                   label=labels[class_val], edgecolors='black', linewidth=0.5)
    
    x1_boundary = np.linspace(-3, 3, 1000)
    x2_boundary = np.sin(4 * x1_boundary) + x1_boundary
    plt.plot(x1_boundary, x2_boundary, 'black', linewidth=3, 
             label='True Decision Boundary', alpha=0.8)
    
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.title(title)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.xlim(-3, 3)
    plt.ylim(-3, 3)
    plt.show()

plot_dataset_with_boundary(X_train, y_train, "Training Dataset with Decision Boundary")
```

### KNN Implementation

```{python}
def euclidean_distance(point1, point2):
    return np.sqrt(np.sum((point1 - point2) ** 2))

def knn_predict_single(X_train, y_train, x_query, k):
    distances = []
    
    for i in range(len(X_train)):
        dist = euclidean_distance(x_query, X_train[i])
        distances.append((dist, y_train[i]))
    
    distances.sort(key=lambda x: x[0])
    k_nearest = distances[:k]
    
    k_labels = [label for _, label in k_nearest]
    
    return max(set(k_labels), key=k_labels.count)

def knn_predict(X_train, y_train, X_test, k):
    predictions = []
    
    for x_query in X_test:
        pred = knn_predict_single(X_train, y_train, x_query, k)
        predictions.append(pred)
    
    return np.array(predictions)

def calculate_accuracy(y_true, y_pred):
    return np.mean(y_true == y_pred)

k_test = 5
predictions = knn_predict(X_train, y_train, X_test, k_test)
accuracy = calculate_accuracy(y_test, predictions)

print(f"KNN (k={k_test}) Accuracy: {accuracy:.4f}")
```

### Comparison with Scikit-learn

```{python}
from sklearn.neighbors import KNeighborsClassifier

sklearn_knn = KNeighborsClassifier(n_neighbors=k_test)
sklearn_knn.fit(X_train, y_train)
sklearn_predictions = sklearn_knn.predict(X_test)
sklearn_accuracy = calculate_accuracy(y_test, sklearn_predictions)

print("Comparison of implementations:")
print(f"Our KNN accuracy: {accuracy:.4f}")
print(f"Sklearn KNN accuracy: {sklearn_accuracy:.4f}")
print(f"Predictions match: {np.array_equal(predictions, sklearn_predictions)}")
```

### Optimal k Analysis

```{python}
def evaluate_knn_performance(X_train, y_train, X_test, y_test, k_range):
    accuracies = []
    
    for k in k_range:
        predictions = knn_predict(X_train, y_train, X_test, k)
        accuracy = calculate_accuracy(y_test, predictions)
        accuracies.append(accuracy)
        
        if k % 5 == 0 or k <= 5:
            print(f"k={k:2d}: Accuracy = {accuracy:.4f}")
    
    return accuracies

k_range = range(1, 31)
print("Evaluating KNN performance for k = 1 to 30:")
accuracies = evaluate_knn_performance(X_train, y_train, X_test, y_test, k_range)

optimal_k = k_range[np.argmax(accuracies)]
max_accuracy = max(accuracies)

print(f"\nOptimal k: {optimal_k}")
print(f"Maximum accuracy: {max_accuracy:.4f}")
```

### Visualization of Results

```{python}
plt.figure(figsize=(12, 8))
plt.plot(k_range, accuracies, 'bo-', linewidth=2, markersize=6)
plt.axvline(x=optimal_k, color='red', linestyle='--', linewidth=2, 
           label=f'Optimal k = {optimal_k}')
plt.axhline(y=max_accuracy, color='red', linestyle='--', linewidth=2, alpha=0.5)

plt.title('KNN Performance: Accuracy vs Number of Neighbors (k)')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Classification Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xlim(1, 30)
plt.ylim(min(accuracies) - 0.05, max(accuracies) + 0.05)

plt.annotate(f'Max Accuracy: {max_accuracy:.4f}', 
            xy=(optimal_k, max_accuracy), 
            xytext=(optimal_k + 5, max_accuracy - 0.03),
            arrowprops=dict(arrowstyle='->', color='red'),
            fontsize=12, color='red')

plt.show()

def plot_decision_boundary(X_train, y_train, X_test, y_test, k, resolution=100):
    x1_min, x1_max = -3, 3
    x2_min, x2_max = -3, 3
    
    x1_grid = np.linspace(x1_min, x1_max, resolution)
    x2_grid = np.linspace(x2_min, x2_max, resolution)
    X1_mesh, X2_mesh = np.meshgrid(x1_grid, x2_grid)
    
    mesh_points = np.c_[X1_mesh.ravel(), X2_mesh.ravel()]
    
    mesh_predictions = knn_predict(X_train, y_train, mesh_points, k)
    mesh_predictions = mesh_predictions.reshape(X1_mesh.shape)
    
    plt.figure(figsize=(12, 10))
    
    plt.contourf(X1_mesh, X2_mesh, mesh_predictions, alpha=0.3, 
                colors=['lightcoral', 'lightblue'], levels=[0, 0.5, 1])
    
    colors = ['red', 'blue']
    labels = ['Class 0', 'Class 1']
    
    for class_val in [0, 1]:
        mask = y_train == class_val
        plt.scatter(X_train[mask, 0], X_train[mask, 1], 
                   c=colors[class_val], alpha=0.8, s=60,
                   label=f'{labels[class_val]} (Train)', 
                   edgecolors='black', linewidth=0.5, marker='o')
    
    for class_val in [0, 1]:
        mask = y_test == class_val
        plt.scatter(X_test[mask, 0], X_test[mask, 1], 
                   c=colors[class_val], alpha=0.8, s=60,
                   label=f'{labels[class_val]} (Test)', 
                   edgecolors='black', linewidth=0.5, marker='^')
    
    x1_boundary = np.linspace(-3, 3, 1000)
    x2_boundary = np.sin(4 * x1_boundary) + x1_boundary
    plt.plot(x1_boundary, x2_boundary, 'black', linewidth=3, 
             label='True Decision Boundary')
    
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.title(f'KNN Decision Boundary (k={k}, Accuracy={accuracies[k-1]:.4f})')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    plt.xlim(-3, 3)
    plt.ylim(-3, 3)
    plt.tight_layout()
    plt.show()

plot_decision_boundary(X_train, y_train, X_test, y_test, optimal_k, resolution=50)
```

### Results Summary

The K-Nearest Neighbors implementation successfully learned the non-linear decision boundary. Key findings:

1. **Algorithm Accuracy**: Our implementation achieved high accuracy and matched scikit-learn's results exactly
2. **Optimal k**: The analysis reveals the optimal number of neighbors that balances bias and variance
3. **Decision Boundary**: KNN effectively approximates the complex sinusoidal boundary using local neighborhoods
4. **Performance Trade-offs**: Lower k values can overfit to noise, while higher k values may oversimplify the boundary

The choice of k represents a fundamental bias-variance trade-off in machine learning:
- **Low k**: Low bias, high variance (sensitive to noise)
- **High k**: High bias, low variance (smoother but potentially less accurate boundaries)

## Conclusion

Both implementations demonstrate the core principles of their respective machine learning paradigms:

- **K-Means** shows how unsupervised learning can discover hidden patterns in data through iterative optimization
- **KNN** illustrates how supervised learning can make predictions using local similarity without explicit model parameters

These algorithms serve as excellent foundations for understanding more complex machine learning methods while remaining interpretable and effective for many real-world applications.


