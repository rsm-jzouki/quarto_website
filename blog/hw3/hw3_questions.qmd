---
title: "Multinomial Logit Model"
author: "Your Name"
date: today
---


This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. 


## 1. Likelihood for the Multi-nomial Logit (MNL) Model

Suppose we have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). 

We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:

$$ U_{ij} = x_j'\beta + \epsilon_{ij} $$

where $\epsilon_{ij}$ is an i.i.d. extreme value error term. 

The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^3e^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$



## 2. Simulate Conjoint Data

We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a "no choice" option; each simulated respondent must select one of the 3 alternatives. 

Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \$4 to \$32 in increments of \$4.

The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is 

$$
u_{ij} = (1 \times Netflix_j) + (0.5 \times Prime_j) + (-0.8*Ads_j) - 0.1\times Price_j + \varepsilon_{ij}
$$

where the variables are binary indicators and $\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.

The following code provides the simulation of the conjoint data.

```{python}
import numpy as np
import pandas as pd
from itertools import product

# set seed for reproducibility
np.random.seed(123)

# define attributes
brands = ["N", "P", "H"]  # Netflix, Prime, Hulu
ads = ["Yes", "No"]
prices = range(8, 33, 4)

# generate all possible profiles
profiles = pd.DataFrame(
    list(product(brands, ads, prices)),
    columns=["brand", "ad", "price"]
)
m = len(profiles)

# assign part-worth utilities (true parameters)
b_util = {"N": 1.0, "P": 0.5, "H": 0.0}
a_util = {"Yes": -0.8, "No": 0.0}
p_util = lambda p: -0.1 * p

# number of respondents, choice tasks, and alternatives per task
n_peeps = 100
n_tasks = 10
n_alts = 3

def sim_one(id):
    datlist = []
    
    # loop over choice tasks
    for t in range(1, n_tasks + 1):
        # randomly sample 3 alts
        sample_idx = np.random.choice(m, size=n_alts, replace=False)
        dat = pd.DataFrame({
            "resp": id,
            "task": t,
            "brand": profiles.iloc[sample_idx]["brand"].values,
            "ad": profiles.iloc[sample_idx]["ad"].values,
            "price": profiles.iloc[sample_idx]["price"].values
        })
        
        # compute deterministic portion of utility
        dat["v"] = (
            dat["brand"].map(b_util) + 
            dat["ad"].map(a_util) + 
            dat["price"].apply(p_util)
        ).round(10)
        
        # add Gumbel noise (Type I extreme value)
        dat["e"] = -np.log(-np.log(np.random.uniform(size=n_alts)))
        dat["u"] = dat["v"] + dat["e"]
        
        # identify chosen alternative
        dat["choice"] = (dat["u"] == dat["u"].max()).astype(int)
        
        datlist.append(dat)
    
    # combine all tasks for one respondent
    return pd.concat(datlist, ignore_index=True)

# simulate data for all respondents
conjoint_data = pd.concat(
    [sim_one(id) for id in range(1, n_peeps + 1)],
    ignore_index=True
)

# keep only observable data
conjoint_data = conjoint_data[["resp", "task", "brand", "ad", "price", "choice"]]
```



## 3. Preparing the Data for Estimation

The "hard part" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.

```{python}
import pandas as pd

# Load the data
data = pd.read_csv("conjoint_data.csv")

# One-hot encode brand (reference: Hulu)
data["is_netflix"] = (data["brand"] == "N").astype(int)
data["is_prime"] = (data["brand"] == "P").astype(int)
# Ad: 1 if Yes, 0 if No
data["has_ads"] = (data["ad"] == "Yes").astype(int)

# Create a set_id for each choice set
data["set_id"] = data["resp"].astype(str) + "_" + data["task"].astype(str)

data.head()
```



## 4. Estimation via Maximum Likelihood

```{python}
import numpy as np

def mnl_log_likelihood(beta, data):
    # beta: array-like, order: [is_netflix, is_prime, has_ads, price]
    # data: DataFrame with columns is_netflix, is_prime, has_ads, price, set_id, choice
    X = data[["is_netflix", "is_prime", "has_ads", "price"]].values
    utilities = X @ beta
    data = data.copy()
    data["utility"] = utilities
    log_lik = 0.0
    for set_id, group in data.groupby("set_id"):
        utils = group["utility"].values
        exp_utils = np.exp(utils - np.max(utils))  # for numerical stability
        probs = exp_utils / exp_utils.sum()
        chosen = group["choice"].values
        # Only one chosen per set, so log(prob) for chosen alt
        log_lik += np.log(probs[chosen == 1][0])
    return log_lik
```

```{python}
from scipy.optimize import minimize
from scipy.linalg import inv
import numpy as np

# Negative log-likelihood for minimization
def neg_mnl_log_likelihood(beta, data):
    return -mnl_log_likelihood(beta, data)

# Initial guess (zeros)
init_params = np.zeros(4)

# Fit the model
result = minimize(
    neg_mnl_log_likelihood,
    init_params,
    args=(data,),
    method='BFGS',
    options={'disp': True}
)

mle_params = result.x
hessian_inv = result.hess_inv
std_errors = np.sqrt(np.diag(hessian_inv))

# 95% confidence intervals
z = 1.96
ci_lower = mle_params - z * std_errors
ci_upper = mle_params + z * std_errors

param_names = ["is_netflix", "is_prime", "has_ads", "price"]
summary = pd.DataFrame({
    "coef": mle_params,
    "std_err": std_errors,
    "ci_lower": ci_lower,
    "ci_upper": ci_upper
}, index=param_names)

print(summary)
```



## 5. Estimation via Bayesian Methods

```{python}
import numpy as np

# Log-prior: N(0,5) for binary betas, N(0,1) for price beta
def log_prior(beta):
    # beta: [is_netflix, is_prime, has_ads, price]
    lp = 0
    lp += -0.5 * (beta[0]**2) / 5 - 0.5 * np.log(2 * np.pi * 5)
    lp += -0.5 * (beta[1]**2) / 5 - 0.5 * np.log(2 * np.pi * 5)
    lp += -0.5 * (beta[2]**2) / 5 - 0.5 * np.log(2 * np.pi * 5)
    lp += -0.5 * (beta[3]**2) / 1 - 0.5 * np.log(2 * np.pi * 1)
    return lp

def log_posterior(beta, data):
    return mnl_log_likelihood(beta, data) + log_prior(beta)

n_steps = 11000
burn = 1000
np.random.seed(42)

# Proposal stddevs: [0.05, 0.05, 0.05, 0.005]
proposal_scales = np.array([0.05, 0.05, 0.05, 0.005])

samples = np.zeros((n_steps, 4))
log_posts = np.zeros(n_steps)

# Start at MLE or zeros
current = np.zeros(4)
current_log_post = log_posterior(current, data)

for t in range(n_steps):
    proposal = current + np.random.normal(0, proposal_scales)
    proposal_log_post = log_posterior(proposal, data)
    accept_prob = np.exp(proposal_log_post - current_log_post)
    if np.random.rand() < accept_prob:
        current = proposal
        current_log_post = proposal_log_post
    samples[t] = current
    log_posts[t] = current_log_post

# Discard burn-in
theta_samples = samples[burn:]
```

```{python}
import matplotlib.pyplot as plt

param_idx = 0  # is_netflix
param_name = "is_netflix"

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(theta_samples[:, param_idx], alpha=0.7)
plt.title(f"Trace plot for {param_name}")
plt.xlabel("Iteration")
plt.ylabel("Parameter value")

plt.subplot(1, 2, 2)
plt.hist(theta_samples[:, param_idx], bins=40, density=True, alpha=0.7)
plt.title(f"Posterior histogram for {param_name}")
plt.xlabel("Parameter value")
plt.ylabel("Density")

plt.tight_layout()
plt.show()
```

```{python}
import numpy as np
import pandas as pd

param_names = ["is_netflix", "is_prime", "has_ads", "price"]

# Posterior summaries
means = theta_samples.mean(axis=0)
stds = theta_samples.std(axis=0)
ci_lower = np.percentile(theta_samples, 2.5, axis=0)
ci_upper = np.percentile(theta_samples, 97.5, axis=0)

posterior_summary = pd.DataFrame({
    "mean": means,
    "std": stds,
    "ci_lower": ci_lower,
    "ci_upper": ci_upper
}, index=param_names)

print("Posterior summary:")
print(posterior_summary)

# Compare to MLE
print("\nMLE summary:")
print(summary)
```



## 6. Discussion

- If the data were not simulated, the parameter estimates would reflect the actual preferences and trade-offs of the sampled population. The estimates might be less close to the 'true' values and could be influenced by sampling variability, omitted variables, or real-world complexities.
- $\beta_\text{Netflix} > \beta_\text{Prime}$ means that, all else equal, consumers have a stronger preference for Netflix over Prime. The larger the coefficient, the higher the utility and probability of choosing that brand.
- A negative $\beta_\text{price}$ is expected and makes sense: as price increases, the utility of an alternative decreases, making it less likely to be chosen.

- To simulate and estimate a multi-level (hierarchical) model, allow each respondent to have their own set of coefficients (part-worths), drawn from a population distribution (e.g., $\beta_i \sim N(\mu, \Sigma)$). In simulation, generate individual-level betas, then simulate choices. For estimation, use hierarchical Bayesian methods (e.g., Gibbs sampling, HMC) or mixed logit models to estimate both the population-level parameters and the distribution of individual-level preferences.











